# efficient-transformer
Experimenting with MIPS approximations to dot-product attention.

Based on the transformer implementation provided in https://nlp.seas.harvard.edu/2018/04/03/attention.html, and a refactoring of the implementation of self-critical sequence training with Transformers in https://github.com/ruotianluo/self-critical.pytorch.
